{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeurIPS Open Polymer Prediction 2025 - EDA\n",
    "\n",
    "Exploratory Data Analysis for polymer property prediction competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_df = pd.read_csv('../data/raw/train.csv')\n",
    "test_df = pd.read_csv('../data/raw/test.csv')\n",
    "sample_submission = pd.read_csv('../data/raw/sample_submission.csv')\n",
    "\n",
    "# Load supplemental datasets\n",
    "supplement_dfs = []\n",
    "for i in range(1, 5):\n",
    "    df = pd.read_csv(f'../data/raw/train_supplement/dataset{i}.csv')\n",
    "    df['source'] = f'dataset{i}'\n",
    "    supplement_dfs.append(df)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Supplemental datasets: {[df.shape for df in supplement_dfs]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target statistics\n",
    "target_columns = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "train_df[target_columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values per target\n",
    "missing_stats = pd.DataFrame({\n",
    "    'Missing Count': train_df[target_columns].isnull().sum(),\n",
    "    'Missing %': (train_df[target_columns].isnull().sum() / len(train_df) * 100).round(2),\n",
    "    'Available Count': train_df[target_columns].notnull().sum(),\n",
    "    'Available %': (train_df[target_columns].notnull().sum() / len(train_df) * 100).round(2)\n",
    "})\n",
    "\n",
    "print(\"\\nMissing Value Statistics:\")\n",
    "print(missing_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "missing_stats[['Available %', 'Missing %']].plot(kind='barh', stacked=True, ax=ax, color=['#2ecc71', '#e74c3c'])\n",
    "ax.set_xlabel('Percentage (%)')\n",
    "ax.set_title('Data Availability by Target')\n",
    "ax.legend(['Available', 'Missing'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for each target\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(target_columns):\n",
    "    data = train_df[col].dropna()\n",
    "    axes[i].hist(data, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[i].set_title(f'{col} Distribution (n={len(data)})')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].axvline(data.mean(), color='red', linestyle='--', label=f'Mean: {data.mean():.3f}')\n",
    "    axes[i].legend()\n",
    "\n",
    "axes[-1].axis('off')  # Hide last subplot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr_matrix = train_df[target_columns].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
    "ax.set_title('Target Correlations')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMILES Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMILES length distribution\n",
    "train_df['smiles_length'] = train_df['SMILES'].str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Length distribution\n",
    "axes[0].hist(train_df['smiles_length'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('SMILES Length')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('SMILES Length Distribution')\n",
    "axes[0].axvline(train_df['smiles_length'].mean(), color='red', linestyle='--',\n",
    "                label=f'Mean: {train_df[\"smiles_length\"].mean():.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Boxplot\n",
    "axes[1].boxplot(train_df['smiles_length'])\n",
    "axes[1].set_ylabel('SMILES Length')\n",
    "axes[1].set_title('SMILES Length Boxplot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"SMILES Length Statistics:\")\n",
    "print(train_df['smiles_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polymer features from SMILES\n",
    "train_df['has_repeating_unit'] = train_df['SMILES'].str.contains('\\*', regex=False)\n",
    "train_df['num_stars'] = train_df['SMILES'].str.count('\\*')\n",
    "train_df['num_rings'] = train_df['SMILES'].str.count('1') + train_df['SMILES'].str.count('2')\n",
    "train_df['num_aromatic'] = train_df['SMILES'].str.count('c')\n",
    "\n",
    "print(f\"\\nPolymer Features:\")\n",
    "print(f\"Samples with repeating units (*): {train_df['has_repeating_unit'].sum()} ({train_df['has_repeating_unit'].mean()*100:.1f}%)\")\n",
    "print(f\"Average number of * symbols: {train_df['num_stars'].mean():.2f}\")\n",
    "print(f\"Average number of rings: {train_df['num_rings'].mean():.2f}\")\n",
    "print(f\"Average aromatic count: {train_df['num_aromatic'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample SMILES Visualization\n",
    "\n",
    "Let's visualize some polymer structures using RDKit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Draw\n",
    "    \n",
    "    # Sample some SMILES\n",
    "    sample_smiles = train_df['SMILES'].head(8).tolist()\n",
    "    \n",
    "    # Convert to molecules\n",
    "    mols = [Chem.MolFromSmiles(s) for s in sample_smiles]\n",
    "    \n",
    "    # Draw\n",
    "    img = Draw.MolsToGridImage(mols, molsPerRow=4, subImgSize=(300, 300))\n",
    "    display(img)\n",
    "except ImportError:\n",
    "    print(\"RDKit not installed. Run: pip install rdkit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplemental Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze supplemental datasets\n",
    "for df in supplement_dfs:\n",
    "    print(f\"\\n{df['source'].iloc[0]}:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    if 'TC_mean' in df.columns:\n",
    "        print(f\"  TC_mean range: [{df['TC_mean'].min():.3f}, {df['TC_mean'].max():.3f}]\")\n",
    "        print(f\"  TC_mean mean: {df['TC_mean'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "**Summary of findings:**\n",
    "\n",
    "1. **Multi-task sparse targets**: Most samples don't have all 5 properties measured\n",
    "2. **SMILES characteristics**: Polymers use `*` to denote repeating units\n",
    "3. **Supplemental data**: Additional datasets provide extra training signal\n",
    "4. **Target correlations**: Some properties may be correlated (useful for multi-task learning)\n",
    "\n",
    "**Modeling recommendations:**\n",
    "- Use multi-task learning with masked loss (ignore missing values)\n",
    "- Leverage pretrained molecular transformers (ChemBERTa)\n",
    "- Consider SMILES augmentation for data augmentation\n",
    "- Use supplemental datasets with domain adaptation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
